{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FD4BEGGyrIk",
        "outputId": "2036cedb-80f7-46ca-e5c0-d2b57d9af436"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os, gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6h_cME-peUI"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1cQYdELPy2F8"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, attention_weights = self.attention(x, x, x)\n",
        "        return output, attention_weights\n",
        "\n",
        "class LSTMModule(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout_prob, device):\n",
        "        super(LSTMModule, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.device = device\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.attention = Attention(hidden_size, 4)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out, attention_weights = self.attention(out)\n",
        "\n",
        "        out = torch.mean(out, dim=1)\n",
        "\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out, attention_weights\n",
        "\n",
        "class CNNModule(nn.Module):\n",
        "    def __init__(self, input_channels, dropout):\n",
        "        super(CNNModule, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        self.attention = Attention(256, 4)\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        x = self.relu1(self.conv1(x))\n",
        "        # print('Shape before pool', 1, x.shape)\n",
        "        x = self.pool1(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.relu2(self.conv2(x))\n",
        "        # print('Shape before pool', 2, x.shape)\n",
        "        x = self.pool2(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.relu3(self.conv3(x))\n",
        "        # print('Shape before pool', 3, x.shape)\n",
        "        x = self.pool3(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x, attention_weights = self.attention(x)\n",
        "\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.squeeze(-1)\n",
        "\n",
        "        return x, attention_weights\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, input_channels, output_size, dropout):\n",
        "        super(CNNModel, self).__init__()\n",
        "\n",
        "        self.cnn_module = CNNModule(input_channels, dropout)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, output_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, attention_weights = self.cnn_module(x)\n",
        "        x = self.fc(x)\n",
        "        return x, attention_weights\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, device, dropout_prob, num_layers=1):\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        self.lstm_module = LSTMModule(input_size, hidden_size, num_layers, dropout_prob, device)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(64, output_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, attention_weights = self.lstm_module(x)\n",
        "        x = self.fc(x)\n",
        "        return x, attention_weights\n",
        "\n",
        "class MultimodalModel(nn.Module):\n",
        "    def __init__(self, ip_size_landmarks, ip_size_aus, ip_size_gaze, hidden_size, output_size, device, dropout_prob, num_layers=1):\n",
        "        super(MultimodalModel, self).__init__()\n",
        "\n",
        "        self.landmarks_lstm = LSTMModule(ip_size_landmarks, hidden_size, num_layers, dropout_prob, device)\n",
        "\n",
        "        self.aus_cnn = CNNModule(ip_size_aus, dropout_prob)\n",
        "        self.gaze_cnn = CNNModule(ip_size_gaze, dropout_prob)\n",
        "\n",
        "        self.combined_attention = Attention(hidden_size + 256 * 2, num_heads=4)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_size + 256 * 2, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(64, output_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, landmarks, aus, gaze):\n",
        "        out_landmarks, attn_weights_landmarks = self.landmarks_lstm(landmarks)\n",
        "        \n",
        "        out_aus, attn_weights_aus = self.aus_cnn(aus)\n",
        "        out_gaze, attn_weights_gaze = self.gaze_cnn(gaze)\n",
        "\n",
        "        combined_out = torch.cat((out_landmarks, out_aus, out_gaze), dim=1).unsqueeze(1)\n",
        "\n",
        "        if self.combined_attn:\n",
        "            combined_out, combined_attention_weights = self.combined_attention(combined_out)\n",
        "\n",
        "        combined_out = combined_out.squeeze(1)\n",
        "\n",
        "        out = self.fc(combined_out)\n",
        "        \n",
        "        if self.combined_attn:\n",
        "            return out, (attn_weights_landmarks, attn_weights_aus, attn_weights_gaze, combined_attention_weights)\n",
        "        else:\n",
        "            return out, (attn_weights_landmarks, attn_weights_aus, attn_weights_gaze)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-iUW_UspW14"
      },
      "source": [
        "# Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tsebfm2L3mnf"
      },
      "outputs": [],
      "source": [
        "class DAICDataset(Dataset):\n",
        "\n",
        "    def __init__(self, split_details, chunk_size, is_train=True, max_workers=4):\n",
        "        self.split_details = pd.read_csv(split_details)\n",
        "        self.is_train = is_train\n",
        "        self.chunk_size = chunk_size\n",
        "        self.base_url = '/content/drive/MyDrive/Dissertation/data/data/'\n",
        "        self.max_workers = max_workers\n",
        "\n",
        "        self.data_cache = {}\n",
        "        self.chunk_info = self._calculate_chunks_parallel()\n",
        "\n",
        "    def __len__(self):\n",
        "        return sum(num_chunks for _, num_chunks in self.chunk_info)\n",
        "\n",
        "    def _load_data(self, pid):\n",
        "        base_path = os.path.join(self.base_url, str(pid))\n",
        "        landmarks = pd.read_csv(f'{base_path}/{pid}_CLNF_features.csv')\n",
        "        aus = pd.read_csv(f'{base_path}/{pid}_CLNF_AUs.csv')\n",
        "        gaze = pd.read_csv(f'{base_path}/{pid}_CLNF_gaze.csv')\n",
        "\n",
        "        valid_indices = landmarks.index.intersection(aus.index).intersection(gaze.index)\n",
        "\n",
        "        landmarks = landmarks.loc[valid_indices].iloc[:, 4:].values.astype(np.float32)\n",
        "        aus = aus.loc[valid_indices].iloc[:, 4:].values.astype(np.float32)\n",
        "        gaze = gaze.loc[valid_indices].iloc[:, 4:].values.astype(np.float32)\n",
        "\n",
        "        return {\n",
        "            'landmarks': landmarks,\n",
        "            'aus': aus,\n",
        "            'gaze': gaze,\n",
        "            'num_frames': len(landmarks)\n",
        "        }\n",
        "\n",
        "    def _calculate_chunks_parallel(self):\n",
        "        chunk_info = []\n",
        "\n",
        "        # parallel load data\n",
        "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "            futures = {executor.submit(self._load_data, int(row['Participant_ID'])): row['Participant_ID']\n",
        "                       for _, row in self.split_details.iterrows()}\n",
        "\n",
        "            for future in as_completed(futures):\n",
        "                pid = futures[future]\n",
        "                data = future.result()\n",
        "                self.data_cache[pid] = data\n",
        "                num_chunks = (data['num_frames'] + self.chunk_size - 1) // self.chunk_size\n",
        "                chunk_info.append((pid, num_chunks))\n",
        "\n",
        "        return chunk_info\n",
        "\n",
        "    def _load_chunk_data(self, pid, chunk_idx):\n",
        "        data = self.data_cache[pid]\n",
        "        start = chunk_idx * self.chunk_size\n",
        "        end = min(start + self.chunk_size, data['num_frames'])\n",
        "\n",
        "        landmarks = data['landmarks'][start:end]\n",
        "        aus = data['aus'][start:end]\n",
        "        gaze = data['gaze'][start:end]\n",
        "\n",
        "        # pad data\n",
        "        if end - start < self.chunk_size:\n",
        "          padding_size = self.chunk_size - (end - start)\n",
        "          landmarks = np.pad(landmarks, ((0, padding_size), (0, 0)), mode='constant')\n",
        "          aus = np.pad(aus, ((0, padding_size), (0, 0)), mode='constant')\n",
        "          gaze = np.pad(gaze, ((0, padding_size), (0, 0)), mode='constant')\n",
        "\n",
        "        sample = {\n",
        "            'pid': pid,\n",
        "            'landmarks': torch.tensor(landmarks, dtype=torch.float32),\n",
        "            'aus': torch.tensor(aus, dtype=torch.float32),\n",
        "            'gaze': torch.tensor(gaze, dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "        if self.is_train:\n",
        "            label = self.split_details[self.split_details['Participant_ID'] == pid]['PHQ8_Binary'].values[0]\n",
        "            sample[\"label\"] = label\n",
        "        else:\n",
        "            actual = self.split_details[self.split_details['Participant_ID'] == pid]['PHQ_Binary'].values[0]\n",
        "            sample[\"actual\"] = actual\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cumulative_chunks = 0\n",
        "        for pid, num_chunks in self.chunk_info:\n",
        "            if idx < cumulative_chunks + num_chunks:\n",
        "                chunk_idx = idx - cumulative_chunks\n",
        "                return self._load_chunk_data(pid, chunk_idx)\n",
        "            cumulative_chunks += num_chunks\n",
        "\n",
        "        raise IndexError(\"Index out of range\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZiooA21phy1"
      },
      "source": [
        "# Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tX0F0PFszB8q"
      },
      "outputs": [],
      "source": [
        "def train_model(config, train_dataset, val_dataset, class_weights):\n",
        "    feature_size = {\"landmarks\": 136, \"aus\": 20, \"gaze\": 12}\n",
        "    method = \"chunk\"\n",
        "    m_name = f\"{config['model']}_{config['feature']}_{method}\"\n",
        "\n",
        "    base_url = '/content/drive/MyDrive/Dissertation/data/'\n",
        "    write_url = '/content/drive/MyDrive/Dissertation/models/'\n",
        "\n",
        "    # initialise model\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if config['model'] == \"lstm\":\n",
        "        model = LSTMModel(input_size=feature_size[config['feature']],\n",
        "                          hidden_size=config['hidden_size'],\n",
        "                          dropout_prob=config['dropout'],\n",
        "                          num_layers=config['lstm_layer'],\n",
        "                          output_size=1,\n",
        "                          device=device)\n",
        "    elif config['model'] == \"cnn\":\n",
        "        model = CNNModel(input_channels=feature_size[config['feature']],\n",
        "                         output_size=1,\n",
        "                         dropout=config['dropout'])\n",
        "    elif config['model'] == \"multimodal\":\n",
        "        model = MultimodalModel(ip_size_landmarks=feature_size['landmarks'],\n",
        "                                ip_size_aus=feature_size['aus'],\n",
        "                                ip_size_gaze=feature_size['gaze'],\n",
        "                                hidden_size=config['hidden_size'],\n",
        "                                dropout_prob=config['dropout'],\n",
        "                                output_size=1,\n",
        "                                device=device)\n",
        "    model.to(device)\n",
        "\n",
        "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config['lr'], weight_decay=1e-5)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
        "\n",
        "    t_precisions, t_recalls ,t_f1s, t_losses = [], [], [], []\n",
        "    v_precisions, v_recalls ,v_f1s, v_losses = [], [], [], []\n",
        "    best_thresholds = []\n",
        "    best_f1 = 0.0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    best_threshold = 0.5\n",
        "    thresholds = np.arange(0.3, 0.7, 0.01)\n",
        "\n",
        "    patience = 10\n",
        "    noimp_count = 0\n",
        "\n",
        "    for epoch in range(config['epochs']):\n",
        "        model.train()\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        print(f'Epoch: {epoch}')\n",
        "\n",
        "        epoch_loss = 0.0\n",
        "        all_labels_train, all_preds_train = [], []\n",
        "\n",
        "        # Training loop\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if config['model'] == \"multimodal\":\n",
        "                landmarks, aus, gaze = batch['landmarks'].to(device), batch['aus'].to(device), batch['gaze'].to(device)\n",
        "                label = batch['label'].to(device).float()\n",
        "                output, attention_weights = model(landmarks, aus, gaze)\n",
        "            else:\n",
        "                data = batch[config['feature']].to(device)\n",
        "                label = batch['label'].to(device).float()\n",
        "                output, attention_weights = model(data)\n",
        "\n",
        "            # print(output)\n",
        "            output = output.squeeze(-1)\n",
        "\n",
        "            # print(output, label)\n",
        "            loss = criterion(output, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            outputs = torch.sigmoid(output)\n",
        "            all_labels_train.extend(label.cpu().numpy())\n",
        "            all_preds_train.extend(outputs.detach().cpu().numpy())\n",
        "\n",
        "        avg_train_loss = epoch_loss / len(train_loader)\n",
        "        t_losses.append(avg_train_loss)\n",
        "\n",
        "        best_epoch_train_f1 = 0.0\n",
        "        best_epoch_train_recall = 0.0\n",
        "        best_epoch_train_precision = 0.0\n",
        "\n",
        "        best_t_train = 0.0\n",
        "\n",
        "        # find best threshold\n",
        "        for threshold in thresholds:\n",
        "            predicted = (all_preds_train > threshold).astype(float)\n",
        "            train_precision = precision_score(all_labels_train, predicted, zero_division=0)\n",
        "            train_recall = recall_score(all_labels_train, predicted)\n",
        "            train_f1 = f1_score(all_labels_train, predicted)\n",
        "\n",
        "            if train_f1 > best_epoch_train_f1:\n",
        "                best_epoch_train_f1 = train_f1\n",
        "                best_epoch_train_recall = train_recall\n",
        "                best_epoch_train_precision = train_precision\n",
        "                best_t_train = threshold\n",
        "\n",
        "        t_f1s.append(best_epoch_train_f1)\n",
        "        t_precisions.append(best_epoch_train_precision)\n",
        "        t_recalls.append(best_epoch_train_recall)\n",
        "\n",
        "        print(f'Training Loss: {avg_train_loss:.4f}, Best Precision: {best_epoch_train_precision:.4f}, Recall: {best_epoch_train_recall:.4f}, F1-Score: {best_epoch_train_f1:.4f}')\n",
        "        print(f\"Best Train Threshold: {best_t_train}\")\n",
        "\n",
        "        # Evaluation loop\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        all_labels_val = []\n",
        "        all_preds_val = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                if config['model'] == \"multimodal\":\n",
        "                    landmarks, aus, gaze = batch['landmarks'].to(device), batch['aus'].to(device), batch['gaze'].to(device)\n",
        "                    label = batch['label'].to(device).float()\n",
        "                    output, attention_weights = model(landmarks, aus, gaze)\n",
        "                else:\n",
        "                    data = batch[config['feature']].to(device)\n",
        "                    label = batch['label'].to(device).float()\n",
        "                    output, attention_weights = model(data)\n",
        "\n",
        "                output = output.squeeze(-1)\n",
        "\n",
        "                val_loss += criterion(output, label).item()\n",
        "                outputs = torch.sigmoid(output)\n",
        "                all_labels_val.extend(label.cpu().numpy())\n",
        "                all_preds_val.extend(outputs.detach().cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        v_losses.append(avg_val_loss)\n",
        "\n",
        "        best_epoch_val_f1 = 0.0\n",
        "        best_epoch_val_recall = 0.0\n",
        "        best_epoch_val_precision = 0.0\n",
        "\n",
        "        # get best threshold\n",
        "        for threshold in thresholds:\n",
        "            predicted = (all_preds_val > threshold).astype(float)\n",
        "            val_precision = precision_score(all_labels_val, predicted, zero_division=0)\n",
        "            val_recall = recall_score(all_labels_val, predicted)\n",
        "            val_f1 = f1_score(all_labels_val, predicted)\n",
        "\n",
        "            if val_f1 > best_epoch_val_f1:\n",
        "                best_epoch_val_f1 = val_f1\n",
        "                best_epoch_val_recall = val_recall\n",
        "                best_epoch_val_precision = val_precision\n",
        "                best_threshold = threshold\n",
        "\n",
        "        v_f1s.append(best_epoch_val_f1)\n",
        "        v_precisions.append(best_epoch_val_precision)\n",
        "        v_recalls.append(best_epoch_val_recall)\n",
        "\n",
        "        best_thresholds.append(best_threshold)\n",
        "\n",
        "        print(f'Validation Loss: {avg_val_loss:.4f}, Best Precision: {best_epoch_val_precision:.4f}, Recall: {best_epoch_val_recall:.4f}, F1-Score: {best_epoch_val_f1:.4f}')\n",
        "        print(f'Best threshold: {best_threshold}')\n",
        "        print()\n",
        "\n",
        "        data = {'model': model,\n",
        "                'optimizer': optimizer,\n",
        "                'best_threshold': best_threshold}\n",
        "\n",
        "\n",
        "        if (best_epoch_val_f1 > best_f1) or (best_epoch_val_f1 == best_f1 and avg_val_loss < best_val_loss):\n",
        "            best_f1 = best_epoch_val_f1\n",
        "            best_val_loss = avg_val_loss\n",
        "            noimp_count = 0\n",
        "            print(f'Best validation Loss: {avg_val_loss:.4f}, F1: {best_f1:.4f}')\n",
        "            _save_model(m_name, epoch, data, best_model=True)\n",
        "        else:\n",
        "            noimp_count += 1\n",
        "\n",
        "        # # uncomment if you want early stopping\n",
        "        # if noimp_count >= patience:\n",
        "        #     print(f\"Early stopping triggered after {epoch} epochs.\")\n",
        "        #     _save_model(m_name, epoch, data)\n",
        "        #     break\n",
        "\n",
        "        if epoch != 0 and ((epoch % 10 == 0) or (epoch == config['epochs'] - 1)):\n",
        "            _save_model(m_name, epoch, data)\n",
        "\n",
        "    pd.DataFrame({\n",
        "        'train_loss': t_losses,\n",
        "        'train_precision': t_precisions,\n",
        "        'train_recall': t_recalls,\n",
        "        'train_f1': t_f1s,\n",
        "        'val_loss': v_losses,\n",
        "        'val_precision': v_precisions,\n",
        "        'val_recall': v_recalls,\n",
        "        'val_f1': v_f1s,\n",
        "        'best_threshold': best_thresholds\n",
        "    }).to_csv(f\"{write_url}{m_name}/progress.csv\", index=False)\n",
        "\n",
        "def _save_model(m_name, epoch, data, best_model=False):\n",
        "    write_url = '/content/drive/MyDrive/Dissertation/models/'\n",
        "\n",
        "    if best_model:\n",
        "        dir = os.path.join(write_url, m_name, \"best_model\")\n",
        "    else:\n",
        "        dir = os.path.join(write_url, m_name, str(epoch))\n",
        "\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "    torch.save(data['model'].state_dict(), os.path.join(dir, \"model.pth\"))\n",
        "    torch.save(data['optimizer'].state_dict(), os.path.join(dir, \"optim.pth\"))\n",
        "    torch.save(data['scheduler'].state_dict(), os.path.join(dir, \"lrscheduler.pth\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeE0MPHXpllJ"
      },
      "source": [
        "# Testing function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3i0Dt7UMzC9F"
      },
      "outputs": [],
      "source": [
        "def test_model(config, test_dataset, class_weights):\n",
        "    feature_size = {\"landmarks\": 136, \"aus\": 20, \"gaze\": 12}\n",
        "    method = \"chunk\"\n",
        "    m_name = f\"{config['model']}_{config['feature']}_{method}\"\n",
        "\n",
        "    base_path = '/content/drive/MyDrive/Dissertation/models/'\n",
        "    model_path = f'{base_path}{m_name}/best_model/model.pth'\n",
        "    progress_path = f'{base_path}{m_name}/progress.csv'\n",
        "\n",
        "    # model loading and initialisation\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if config['model'] == \"lstm\":\n",
        "        model = LSTMModel(input_size=feature_size[config['feature']],\n",
        "                          hidden_size=config['hidden_size'],\n",
        "                          dropout_prob=config['dropout'],\n",
        "                          output_size=1,\n",
        "                          num_layers=config['lstm_layer'],\n",
        "                          device=device)\n",
        "    elif config['model'] == \"cnn\":\n",
        "        model = CNNModel(input_channels=feature_size[config['feature']],\n",
        "                         output_size=1,\n",
        "                         dropout=config['dropout'])\n",
        "    elif config['model'] == \"multimodal\":\n",
        "        model = MultimodalModel(ip_size_landmarks=feature_size['landmarks'],\n",
        "                                ip_size_aus=feature_size['aus'],\n",
        "                                ip_size_gaze=feature_size['gaze'],\n",
        "                                hidden_size=config['hidden_size'],\n",
        "                                dropout_prob=config['dropout'],\n",
        "                                output_size=1,\n",
        "                                device=device)\n",
        "    model.to(device)\n",
        "\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "    # get best threshold from progress.csv file\n",
        "    progress_df = pd.read_csv(progress_path)\n",
        "\n",
        "    best_model_row = None\n",
        "    best_f1 = 0.0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for index, row in progress_df.iterrows():\n",
        "        epoch_val_f1 = row['val_f1']\n",
        "        avg_val_loss = row['val_loss']\n",
        "\n",
        "        if (epoch_val_f1 > best_f1) or (epoch_val_f1 == best_f1 and avg_val_loss < best_val_loss):\n",
        "            best_f1 = epoch_val_f1\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_model_row = row\n",
        "\n",
        "    best_threshold = best_model_row['best_threshold']\n",
        "\n",
        "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    all_preds, all_actuals = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            if config['model'] == \"multimodal\":\n",
        "                landmarks, aus, gaze = batch['landmarks'].to(device), batch['aus'].to(device), batch['gaze'].to(device)\n",
        "                actual = batch['actual'].to(device).float()\n",
        "                output, _ = model(landmarks, aus, gaze)\n",
        "            else:\n",
        "                data = batch[config['feature']].to(device)\n",
        "                actual = batch['actual'].to(device).float()\n",
        "                output, _ = model(data)\n",
        "\n",
        "            output = output.squeeze(-1)\n",
        "            test_loss += criterion(output, actual).item()\n",
        "\n",
        "            output = torch.sigmoid(output)\n",
        "            pred = (output > best_threshold).float()\n",
        "\n",
        "            all_preds.extend(pred.detach().cpu().numpy())\n",
        "            all_actuals.extend(actual.cpu().numpy())\n",
        "\n",
        "    precision = precision_score(all_actuals, all_preds)\n",
        "    recall = recall_score(all_actuals, all_preds)\n",
        "    f1 = f1_score(all_actuals, all_preds)\n",
        "\n",
        "    avg_loss = test_loss / len(test_loader)\n",
        "\n",
        "    print(f'Test Loss: {avg_loss:.4f}')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'Recall: {recall:.4f}')\n",
        "    print(f'F1-Score: {f1:.4f}')\n",
        "\n",
        "    result = {\n",
        "        'model_comb': m_name,\n",
        "        'test_loss': avg_loss,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    }\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG2gw4nVpnrO"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRz1B98Om2Bg"
      },
      "source": [
        "### Initialise variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "57yaJI0J_7gF"
      },
      "outputs": [],
      "source": [
        "models = ['multimodal'] #, 'lstm', 'cnn', 'multimodal'\n",
        "features = ['landmarks', 'aus', 'gaze']\n",
        "\n",
        "config = {\n",
        "    \"model\": \"lstm\",\n",
        "    \"feature\": \"landmarks\",\n",
        "    \"dropout\": 0.2,\n",
        "    \"lr\": 0.001,\n",
        "    \"batch_size\": 8,\n",
        "    \"hidden_size\": 32,\n",
        "    \"epochs\": 30,\n",
        "    \"chunk_size\": 500,\n",
        "    \"lstm_layer\": 3\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BQ0CNrWm4Rr"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZKPcDu2cISl",
        "outputId": "5aea01ec-949e-4b8f-990e-6172cf5e4cbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value Counts:\n",
            " {0: 77, 1: 30}\n",
            "[0.69480519 1.78333333]\n"
          ]
        }
      ],
      "source": [
        "base_url = '/content/drive/MyDrive/Dissertation/data'\n",
        "\n",
        "train_labels_dir = f\"{base_url}/train_split_Depression_AVEC2017.csv\"\n",
        "val_labels_dir = f\"{base_url}/dev_split_Depression_AVEC2017.csv\"\n",
        "\n",
        "train_dataset = DAICDataset(train_labels_dir, config['chunk_size'], is_train=True)\n",
        "val_dataset = DAICDataset(val_labels_dir, config['chunk_size'], is_train=True)\n",
        "\n",
        "train_labels = pd.read_csv(f\"{base_url}/train_split_Depression_AVEC2017.csv\")['PHQ8_Binary'].values\n",
        "# print(np.unique(train_labels))\n",
        "unique, counts = np.unique(train_labels, return_counts=True)\n",
        "\n",
        "value_counts = dict(zip(unique, counts))\n",
        "\n",
        "print(\"Value Counts:\\n\", value_counts)\n",
        "\n",
        "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_labels), y=train_labels)\n",
        "print(class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iZwMEtvmwYx"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU51PrvB-6-t",
        "outputId": "a8976c33-0e8e-4430-e704-d8e6861ef54c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Training Loss: 0.8329, Best Precision: 0.2803, Recall: 0.9634, F1-Score: 0.4342\n",
            "Best Train Threshold: 0.3\n",
            "Validation Loss: 0.9299, Best Precision: 0.3856, Recall: 1.0000, F1-Score: 0.5565\n",
            "Best threshold: 0.4100000000000001\n",
            "\n",
            "Best validation Loss: 0.9299, F1: 0.5565\n",
            "Epoch: 1\n",
            "Training Loss: 0.8225, Best Precision: 0.2828, Recall: 0.9686, F1-Score: 0.4377\n",
            "Best Train Threshold: 0.3\n",
            "Validation Loss: 0.9028, Best Precision: 0.3865, Recall: 1.0000, F1-Score: 0.5575\n",
            "Best threshold: 0.35000000000000003\n",
            "\n",
            "Best validation Loss: 0.9028, F1: 0.5575\n",
            "Epoch: 2\n",
            "Training Loss: 0.8046, Best Precision: 0.3131, Recall: 0.8217, F1-Score: 0.4535\n",
            "Best Train Threshold: 0.38000000000000006\n",
            "Validation Loss: 1.0248, Best Precision: 0.4032, Recall: 0.7646, F1-Score: 0.5280\n",
            "Best threshold: 0.31\n",
            "\n",
            "Epoch: 3\n",
            "Training Loss: 0.7119, Best Precision: 0.4604, Recall: 0.6748, F1-Score: 0.5473\n",
            "Best Train Threshold: 0.4100000000000001\n",
            "Validation Loss: 1.8186, Best Precision: 0.3259, Recall: 0.4633, F1-Score: 0.3826\n",
            "Best threshold: 0.3\n",
            "\n",
            "Epoch: 4\n",
            "Training Loss: 0.6282, Best Precision: 0.6341, Recall: 0.6280, F1-Score: 0.6310\n",
            "Best Train Threshold: 0.5000000000000002\n",
            "Validation Loss: 1.2942, Best Precision: 0.2954, Recall: 0.3975, F1-Score: 0.3389\n",
            "Best threshold: 0.3\n",
            "\n",
            "Epoch: 5\n",
            "Training Loss: 0.5478, Best Precision: 0.6993, Recall: 0.6652, F1-Score: 0.6818\n",
            "Best Train Threshold: 0.5300000000000002\n",
            "Validation Loss: 1.8338, Best Precision: 0.3451, Recall: 0.4076, F1-Score: 0.3738\n",
            "Best threshold: 0.3\n",
            "\n",
            "Epoch: 6\n",
            "Training Loss: 0.5059, Best Precision: 0.7001, Recall: 0.7383, F1-Score: 0.7187\n",
            "Best Train Threshold: 0.48000000000000015\n",
            "Validation Loss: 1.8557, Best Precision: 0.2866, Recall: 0.2278, F1-Score: 0.2539\n",
            "Best threshold: 0.3\n",
            "\n",
            "Epoch: 7\n",
            "Training Loss: 0.4629, Best Precision: 0.7421, Recall: 0.7383, F1-Score: 0.7402\n",
            "Best Train Threshold: 0.5300000000000002\n",
            "Validation Loss: 3.0383, Best Precision: 0.3079, Recall: 0.2405, F1-Score: 0.2701\n",
            "Best threshold: 0.3\n",
            "\n",
            "Epoch: 8\n",
            "Training Loss: 0.4246, Best Precision: 0.7551, Recall: 0.7813, F1-Score: 0.7680\n",
            "Best Train Threshold: 0.5200000000000002\n",
            "Validation Loss: 3.2044, Best Precision: 0.3124, Recall: 0.2709, F1-Score: 0.2902\n",
            "Best threshold: 0.3\n",
            "\n",
            "Epoch: 9\n",
            "Training Loss: 0.3907, Best Precision: 0.7538, Recall: 0.8307, F1-Score: 0.7904\n",
            "Best Train Threshold: 0.49000000000000016\n",
            "Validation Loss: 2.3980, Best Precision: 0.3260, Recall: 0.3392, F1-Score: 0.3325\n",
            "Best threshold: 0.3\n",
            "\n",
            "Epoch: 10\n",
            "Training Loss: 0.3529, Best Precision: 0.7973, Recall: 0.8326, F1-Score: 0.8146\n",
            "Best Train Threshold: 0.5100000000000002\n",
            "Validation Loss: 4.0945, Best Precision: 0.3052, Recall: 0.2241, F1-Score: 0.2584\n",
            "Best threshold: 0.32\n",
            "\n",
            "Epoch: 11\n",
            "Training Loss: 0.3336, Best Precision: 0.8007, Recall: 0.8480, F1-Score: 0.8237\n",
            "Best Train Threshold: 0.5400000000000003\n",
            "Validation Loss: 3.2302, Best Precision: 0.3305, Recall: 0.2924, F1-Score: 0.3103\n",
            "Best threshold: 0.3\n",
            "\n",
            "Epoch: 12\n",
            "Training Loss: 0.2842, Best Precision: 0.8531, Recall: 0.8570, F1-Score: 0.8550\n",
            "Best Train Threshold: 0.5800000000000003\n",
            "Validation Loss: 4.0485, Best Precision: 0.3516, Recall: 0.4063, F1-Score: 0.3770\n",
            "Best threshold: 0.32\n",
            "\n",
            "Epoch: 13\n",
            "Training Loss: 0.2715, Best Precision: 0.8503, Recall: 0.8704, F1-Score: 0.8602\n",
            "Best Train Threshold: 0.5900000000000003\n",
            "Validation Loss: 3.0643, Best Precision: 0.3063, Recall: 0.2772, F1-Score: 0.2910\n",
            "Best threshold: 0.3\n",
            "\n",
            "Epoch: 14\n",
            "Training Loss: 0.2888, Best Precision: 0.8748, Recall: 0.8422, F1-Score: 0.8582\n",
            "Best Train Threshold: 0.6400000000000003\n",
            "Validation Loss: 2.3936, Best Precision: 0.3475, Recall: 0.2481, F1-Score: 0.2895\n",
            "Best threshold: 0.3\n",
            "\n",
            "Epoch: 15\n",
            "Training Loss: 0.2442, Best Precision: 0.8717, Recall: 0.8801, F1-Score: 0.8758\n",
            "Best Train Threshold: 0.6000000000000003\n",
            "Validation Loss: 3.5850, Best Precision: 0.3414, Recall: 0.3392, F1-Score: 0.3403\n",
            "Best threshold: 0.31\n",
            "\n",
            "Epoch: 16\n",
            "Training Loss: 0.2178, Best Precision: 0.8727, Recall: 0.8884, F1-Score: 0.8805\n",
            "Best Train Threshold: 0.5900000000000003\n",
            "Validation Loss: 3.4015, Best Precision: 0.3138, Recall: 0.2848, F1-Score: 0.2986\n",
            "Best threshold: 0.3\n",
            "\n",
            "Epoch: 17\n",
            "Training Loss: 0.2268, Best Precision: 0.8439, Recall: 0.9294, F1-Score: 0.8846\n",
            "Best Train Threshold: 0.47000000000000014\n",
            "Validation Loss: 3.0575, Best Precision: 0.3477, Recall: 0.3886, F1-Score: 0.3670\n",
            "Best threshold: 0.3\n",
            "\n",
            "Epoch: 18\n",
            "Training Loss: 0.1985, Best Precision: 0.8834, Recall: 0.9140, F1-Score: 0.8985\n",
            "Best Train Threshold: 0.5300000000000002\n",
            "Validation Loss: 3.2622, Best Precision: 0.3875, Recall: 0.3532, F1-Score: 0.3695\n",
            "Best threshold: 0.31\n",
            "\n",
            "Epoch: 19\n",
            "Training Loss: 0.2017, Best Precision: 0.8554, Recall: 0.9410, F1-Score: 0.8962\n",
            "Best Train Threshold: 0.48000000000000015\n",
            "Validation Loss: 4.3535, Best Precision: 0.3698, Recall: 0.3722, F1-Score: 0.3710\n",
            "Best threshold: 0.32\n",
            "\n",
            "Epoch: 20\n",
            "Training Loss: 0.1888, Best Precision: 0.9178, Recall: 0.8954, F1-Score: 0.9065\n",
            "Best Train Threshold: 0.6400000000000003\n",
            "Validation Loss: 6.2115, Best Precision: 0.3436, Recall: 0.2823, F1-Score: 0.3099\n",
            "Best threshold: 0.3\n",
            "\n",
            "Epoch: 21\n",
            "Training Loss: 0.1869, Best Precision: 0.9057, Recall: 0.9115, F1-Score: 0.9086\n",
            "Best Train Threshold: 0.5500000000000003\n",
            "Validation Loss: 3.4224, Best Precision: 0.3688, Recall: 0.4785, F1-Score: 0.4165\n",
            "Best threshold: 0.3\n",
            "\n",
            "Epoch: 22\n",
            "Training Loss: 0.1745, Best Precision: 0.9094, Recall: 0.9140, F1-Score: 0.9117\n",
            "Best Train Threshold: 0.6100000000000003\n",
            "Validation Loss: 6.3677, Best Precision: 0.3395, Recall: 0.2570, F1-Score: 0.2925\n",
            "Best threshold: 0.3\n",
            "\n",
            "Epoch: 23\n",
            "Training Loss: 0.1564, Best Precision: 0.9303, Recall: 0.9070, F1-Score: 0.9185\n",
            "Best Train Threshold: 0.6600000000000004\n",
            "Validation Loss: 3.5320, Best Precision: 0.3430, Recall: 0.4329, F1-Score: 0.3828\n",
            "Best threshold: 0.3\n",
            "\n",
            "Epoch: 24\n",
            "Training Loss: 0.1352, Best Precision: 0.9274, Recall: 0.9339, F1-Score: 0.9306\n",
            "Best Train Threshold: 0.6000000000000003\n",
            "Validation Loss: 4.4671, Best Precision: 0.3937, Recall: 0.3937, F1-Score: 0.3937\n",
            "Best threshold: 0.3\n",
            "\n",
            "Epoch: 25\n",
            "Training Loss: 0.1597, Best Precision: 0.9162, Recall: 0.9326, F1-Score: 0.9243\n",
            "Best Train Threshold: 0.6100000000000003\n",
            "Validation Loss: 4.2826, Best Precision: 0.3430, Recall: 0.3608, F1-Score: 0.3516\n",
            "Best threshold: 0.3\n",
            "\n",
            "Epoch: 26\n",
            "Training Loss: 0.1465, Best Precision: 0.9158, Recall: 0.9352, F1-Score: 0.9254\n",
            "Best Train Threshold: 0.5900000000000003\n",
            "Validation Loss: 4.3044, Best Precision: 0.3124, Recall: 0.1949, F1-Score: 0.2401\n",
            "Best threshold: 0.3\n",
            "\n",
            "Epoch: 27\n",
            "Training Loss: 0.1544, Best Precision: 0.9187, Recall: 0.9346, F1-Score: 0.9266\n",
            "Best Train Threshold: 0.5700000000000003\n",
            "Validation Loss: 4.4581, Best Precision: 0.3549, Recall: 0.3190, F1-Score: 0.3360\n",
            "Best threshold: 0.3\n",
            "\n",
            "Epoch: 28\n",
            "Training Loss: 0.1522, Best Precision: 0.9265, Recall: 0.9384, F1-Score: 0.9324\n",
            "Best Train Threshold: 0.5400000000000003\n",
            "Validation Loss: 3.7868, Best Precision: 0.3500, Recall: 0.5038, F1-Score: 0.4131\n",
            "Best threshold: 0.3\n",
            "\n",
            "Epoch: 29\n",
            "Training Loss: 0.1145, Best Precision: 0.9435, Recall: 0.9423, F1-Score: 0.9429\n",
            "Best Train Threshold: 0.6100000000000003\n",
            "Validation Loss: 4.6395, Best Precision: 0.3262, Recall: 0.3304, F1-Score: 0.3283\n",
            "Best threshold: 0.3\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for model in models:\n",
        "    config['model'] = model\n",
        "    if model == 'multimodal':\n",
        "        config['feature'] = 'all'\n",
        "        train_model(config, train_dataset, val_dataset, class_weights)\n",
        "    else:\n",
        "        for feature in features:\n",
        "            print(model, feature)\n",
        "            config['feature'] = feature\n",
        "            train_model(config, train_dataset, val_dataset, class_weights)\n",
        "\n",
        "# train_model(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqQ6LIk4mzKa"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "92eGvf4Unl43"
      },
      "outputs": [],
      "source": [
        "test_labels_dir = \"/content/drive/MyDrive/Dissertation/data/full_test_split.csv\"\n",
        "test_dataset = DAICDataset(test_labels_dir, config['chunk_size'], is_train=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "\n",
        "test_labels = pd.read_csv(test_labels_dir)['PHQ_Binary'].values\n",
        "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(test_labels), y=test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_SvjHAq-szP",
        "outputId": "034cc825-0260-4552-9d80-e8e3932261e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.8445\n",
            "Precision: 0.3133\n",
            "Recall: 0.9941\n",
            "F1-Score: 0.4764\n"
          ]
        }
      ],
      "source": [
        "# test(config)\n",
        "all_results = []\n",
        "\n",
        "for model in models:\n",
        "    config['model'] = model\n",
        "    if model == 'multimodal':\n",
        "        config['feature'] = 'all'\n",
        "        result = test_model(config, test_dataset, class_weights)\n",
        "        all_results.append(result)\n",
        "    else:\n",
        "      for feature in features:\n",
        "          print(model, feature)\n",
        "          config['feature'] = feature\n",
        "\n",
        "          result = test_model(config, test_dataset, class_weights)\n",
        "          all_results.append(result)\n",
        "\n",
        "# Save the results\n",
        "results_df = pd.DataFrame(all_results)\n",
        "csv_output_path = '/content/drive/MyDrive/Dissertation/models/test_results.csv'\n",
        "\n",
        "if os.path.exists(csv_output_path):\n",
        "    existing_df = pd.read_csv(csv_output_path)\n",
        "    combined_df = pd.concat([existing_df, results_df], ignore_index=True)\n",
        "    combined_df.to_csv(csv_output_path, index=False)\n",
        "else:\n",
        "    results_df.to_csv(csv_output_path, index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
